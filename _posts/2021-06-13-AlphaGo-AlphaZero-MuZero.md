---
layout:  post
title:      AlphaGo AlphaZero MuZero
subtitle:  MuZero
date:      2021-06-13  19:54:00
author:     wangmin
header-img: img/home-bg-art.jpg
catalog: true
tags: DeepMind Alpha-Go
---

# AlphaGo, AlphaZero, MuZero



## Mastering Atari, Go, chess and shogi by planning with a learned model

1. 构建一个agent具有规划的能力一直是AI中给一个主要挑战
2. 在有simulator的情况下，Tree based planning methods success，但是在现实过程中，管理环境的动态是非常复杂和未知。
3. lookahead search have achieved remarkable successes in artificial intelligence.
4. RL 2 principal categories:
   1. **Model-based reinforcement learning** (RL)11 aims to address this issue
      1. 典型做法Markov decision process(MDP)
         1. 典型的方法是讲model构建为一个MDP
            1. state transition model: 给定action的情况下，预测下一个状态：
            2. reward model: 在状态转移过程中的reward.
         2. MDP planning algorithms: compute the optimal value function or optimal policy for MDP。
            1. value iteration
            2. Monte Carlo tree search
      2. first learning a model of the environment’s dynamics
         1. 1. 
         2. reconstructing the true environmental state
         3. the sequance of full observations
      3. then planning with respect to the learned model
   2. **model-free RL**方法在Atari 2600上面表现不错
      1. 直接与环境进行交互，来学习optimal policy or value fucntions
      2. 但是这种方式在要求精确的或者复杂的lookahead的领域表现不好。
5. Here we present the MuZero algorithm, which, by combining a **tree-based search** with **a learned model**, achieves superhuman performance in a range of challenging and visually complex domains, **without any knowledge of their underlying dynamics.**
   1. Based on AlphaZero, Powerful search and policy iteration algorithms, but incroporates a learned model into the **training procedure**????
   2. Broader set of environments including signle agent domains and non-zero rewards at intermediate time steps.
   3. **Main Ideas**: predict those aspects of the future that are directly relevant for planning.
      1. 输入当前画面，然后转化为hidden state
      2. at each step, the model produces a policy, value fucntion , immdediate reward prediction
      3. model it trained end to end with the sole objective of accurately estimante these 3 import quantities, to match the improved policy and value function generated by search, as well as the observed reward.
      4. 没有要求模型要精确的记录所有的**hidden state**, 那么就可以丢去很多冗余的信息。
      5. Instead, the hidden states are free to represent any state that correctly estimates the policy, value function and reward。
6. Other Methods
   1. model-based planning approaches
   2. Tree-based planning approaches

## Mastering the game of Go with deep neural networks and tree search







1. State

2. Action

3. Agent

4. Policy $\pi$: 

   1. Policy function $\pi(a|s) \in [0, 1]$
      $$
      \pi(a|s) = P(A = a | S = s)
      $$

   2. it is the probability of taking action A = a given $s$, e.g.,
      1. $\pi(left | s) = 0.2$
      2. $\pi(right | s) = 0.1$
      3. $\pi(up | s) = 0.7$
   3.  Upon oberving state S = s, the agent's action A can be random

5. Reward

6. State transition

   1. state transition can be random
   2. Rondomness is from the environment
   3. $p(s\prime|s, a) = P(S\prime=s\prime|S=s, A= a)$

7. Randomness is RL

   1. Action have randomness
   2. State transition have randomness

8. Rewards

9. Returns 

   1. aka cumulative future reward

      $U_t = R_t + R_{t+1} + R_{t+2} + R_{t+3} + ...$

   2. Discounted return: aka cumulative siscounted future reward
      1. $U_t = R_t + \gamma^1 R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + ...$

10. **Action-Value Function**

    1. Definition: Action-Value function for policy $\pi$

       1. $Q_{\pi}(s_t, a_t) = E[U_t|S_t = s_t, A_t = a_t]$

          For policy $\pi, Q_{\pi}(s, a)$ evaluates how good it is for an agent to pick action $a$ while being in state $s$ 

    2. Definition: Optimal Action-value function

       1. $Q^*(s_t, a_t) = max_{\pi}Q_{pi}(s_t, a_t)$

11. **State-value function**

    1. $V_{\pi}(s_t) = E_A[Q_{\pi}(s_t, A)] = \sum_{a}\pi (a | s_t)*Q_{\pi}(s_t, a)$

       For fixed policy $\pi, V_{\pi}(s)$ evaluates how good the situation is in state $s$

       $E_s[V_{\pi}(S)]$ evaluates how good the policy $\pi$ is.

12. Example
    1. how does AI control the agent?
       1. SUppose we have a good policy $\pi (a | s)$
          1. Upon observe the state $s_t$
          2. random sampling : $a_t \~ () $